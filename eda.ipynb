{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be07ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the dataset root path\n",
    "data_path = \"dataset\"\n",
    "\n",
    "# Define the path to the aspect data folder\n",
    "aspect_data_path = os.path.join(data_path, \"aspect_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df99ec69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in aspect data: 25986\n",
      "\n",
      "Example record:\n",
      "{'id': 'ICLR_2020_1123', 'text': \"This work studies the predictive uncertainty issue of deep learning models . In particular , this work focuses on the distributional uncertainty which is caused by distributional mismatch between training and test examples . The proposed method is developed based on the existing work called Dirichlet Prior Network ( DPN ) . It aims to address the issue of DPN that its loss function is complicated and makes the optimization difficult . Instead , this paper proposes a new loss function for DPN , which consists of the commonly used cross-entropy loss term and a regularization term . Two loss functions are respectively defined over in-domain training examples and out-of-distribution ( OOD ) training examples . The final objective function is a weighted combination of the two loss functions . Experimental study is conducted on one synthetic dataset and two image datasets ( CIFAR-10 and CIFAR-100 ) to demonstrate the properties of the proposed method and compare its performance with the relevant ones in the literature . The issue researched in this work is of significance because understanding the predictive uncertainty of a deep learning model has its both theoretical and practical value . The motivation , research issues and the proposed method are overall clearly presented . The current recommendation is Weak Reject because the experimental study is not convincing or comprehensive enough . 1 .Although the goal of this work is to deal with the inefficiency issue of the objective function of existing DPN with the newly proposed one , this experimental study does not seem to conduct sufficient experiments to demonstrate the advantages ( say , in terms of training efficiency & the capability in making the network scalable for more challenging dataset ) of the proposed objective function over the existing one ; 2 . Table 1 compares the proposed method with ODIN . However , as indicated in this work , ODIN is trained with in-domain examples only . Is this comparison fair ? Actually , ODIN 's setting seems to be more practical and more challenging than the setting used by the propose methods . 3 .The evaluation criteria shall be better explained at the beginning of the experiment , especially how they can be collectively used to verify that the proposed method can better distinguish distributional uncertainty from other uncertainty types . 4 .In addition , the experimental study can be clearer on the training and test splits . How many samples from CIFAR-10 and CIFAR-100 are used for training and test purpose , respectively ? Also , since training examples are from CIFAR-10 and CIFAR-100 and the test examples are also from these two datasets , does this contradict with the motivation of “ distributional mismatch between training and test examples ” mentioned in the abstract ? 5 .The experimental study can have more comparison on challenging datasets with more classes since it is indicated that DPN has difficulty in dealing with a large number of classes . Minor : 1 . Please define the \\\\hat\\\\theta in Eq . ( 5 ) .Also , is the dirac delta estimation a good enough approximation here ? 2 .The \\\\lambda_ { out } < \\\\lambda_ { in } in Eq . ( 11 ) needs to be better explained . In particular , are the first terms in Eq . ( 10 ) and Eq . ( 11 ) comparable in terms of magnitude ? Otherwise , \\\\lambda_ { out } < \\\\lambda_ { in } may not make sense . 3 .The novelty and significance of fine-tuning the proposed model with noisy OOD training images can be better justified .\", 'labels': [[0, 1029, 'summary'], [1030, 1201, 'motivation_positive'], [1204, 1292, 'clarity_positive'], [1293, 1409, 'soundness_negative'], [1555, 1656, 'substance_negative'], [1973, 1998, 'meaningful_comparison_negative'], [3389, 3508, 'soundness_negative']]}\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the aspect data file (correct extension .jsonl)\n",
    "aspect_file_path = os.path.join(aspect_data_path, \"review_with_aspect.jsonl\")\n",
    "\n",
    "# Load aspect data from JSONL file\n",
    "aspect_data = []\n",
    "with open(aspect_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        aspect_data.append(json.loads(line))\n",
    "\n",
    "# Print basic information\n",
    "print(\"Number of records in aspect data:\", len(aspect_data))\n",
    "print(\"\\nExample record:\")\n",
    "print(aspect_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e602af88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect label distribution:\n",
      "summary: 23946\n",
      "motivation_positive: 11121\n",
      "clarity_positive: 13095\n",
      "soundness_negative: 15662\n",
      "substance_negative: 11717\n",
      "meaningful_comparison_negative: 9242\n",
      "soundness_positive: 12780\n",
      "clarity_negative: 15697\n",
      "originality_negative: 8745\n",
      "originality_positive: 11606\n",
      "replicability_negative: 4548\n",
      "substance_positive: 4669\n",
      "meaningful_comparison_positive: 1488\n",
      "replicability_positive: 271\n",
      "motivation_negative: 3499\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Collect all aspect labels\n",
    "all_labels = []\n",
    "for record in aspect_data:\n",
    "    for label in record[\"labels\"]:\n",
    "        all_labels.append(label[2])  # label[2] is the aspect name\n",
    "\n",
    "# Count the frequency of each aspect label\n",
    "label_counter = Counter(all_labels)\n",
    "\n",
    "# Display aspect label distribution\n",
    "print(\"Aspect label distribution:\")\n",
    "for label, count in label_counter.items():\n",
    "    print(f\"{label}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c983a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 25986\n",
      "Total number of unique submissions: 8742\n"
     ]
    }
   ],
   "source": [
    "# Collect all submission IDs\n",
    "submission_ids = [record[\"id\"] for record in aspect_data]\n",
    "\n",
    "# Find the number of unique submissions\n",
    "unique_submissions = set(submission_ids)\n",
    "\n",
    "print(\"Total number of reviews:\", len(submission_ids))\n",
    "print(\"Total number of unique submissions:\", len(unique_submissions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd76c10",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "The dataset contains 8,742 unique submissions (papers).  \n",
    "Across these submissions, there are a total of 25,986 review snippets available.\n",
    "\n",
    "This implies that, on average, there are approximately 3 review snippets per submission (25986 ÷ 8742 ≈ 2.97).\n",
    "\n",
    "This finding is particularly important for two reasons:\n",
    "- **Retrieval phase:** When retrieving past review information for a given submission, there is a relatively large pool of examples available, enhancing the retrieval quality.\n",
    "- **Model training:** During the model training phase, each submission can be associated with multiple review examples, which could lead to richer and more informed decision-making during the accept/reject prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3ddf1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length (in characters): 2319.60\n",
      "Average review length (in words): 419.81\n"
     ]
    }
   ],
   "source": [
    "# Calculate character and word lengths of each review\n",
    "char_lengths = [len(record[\"text\"]) for record in aspect_data]\n",
    "word_lengths = [len(record[\"text\"].split()) for record in aspect_data]\n",
    "\n",
    "# Calculate averages\n",
    "avg_char_length = sum(char_lengths) / len(char_lengths)\n",
    "avg_word_length = sum(word_lengths) / len(word_lengths)\n",
    "\n",
    "print(f\"Average review length (in characters): {avg_char_length:.2f}\")\n",
    "print(f\"Average review length (in words): {avg_word_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b71fe37",
   "metadata": {},
   "source": [
    "### Review Length Analysis\n",
    "\n",
    "The average length of a review snippet is approximately **2,319.60 characters** and **419.81 words**.\n",
    "\n",
    "This suggests that the peer reviews are generally rich and detailed.  \n",
    "Such length ensures that during retrieval, the model will have access to informative and contextually rich snippets.  \n",
    "However, the relatively long sequence length should be considered when designing the model architecture, especially if sequence length limitations (e.g., in transformer models) are a factor.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
